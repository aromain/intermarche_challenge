{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents:\n",
    "0. [Imports & fix random seed](#0)\n",
    "1. [Load data](#1)\n",
    "2. [Pre-processing](#2)\n",
    "3. [Modelling](#3)\n",
    "4. [Post-processing](#4)\n",
    "5. [Create & dump submission file](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Imports & fix random seed <a name=0></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manomano/.pyenv/versions/3.7.0/envs/im_challenge/lib/python3.7/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "SEED = 1337\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "SAMPLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Load data <a name=1></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = pd.read_csv(\"data/ventes_2018.csv\")\n",
    "df_orders.columns = [x.lower() for x in df_orders.columns]\n",
    "\n",
    "df_nom = pd.read_csv(\"data/nomenclature_produits.csv\")\n",
    "df_nom.columns = [x.lower() for x in df_nom.columns]\n",
    "\n",
    "df_pdv = pd.read_csv(\"data/points_de_vente.csv\")\n",
    "df_pdv.columns = [x.lower() for x in df_pdv.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAMPLE:\n",
    "    df_orders = df_orders[df_orders.id_pdv == 124].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Pre-processing <a name=2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create df_train & df_test (1 row by date X id_pdv X id_artc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame()\n",
    "df_test = pd.DataFrame()\n",
    "\n",
    "df_train[\"date\"] = pd.date_range(\"2018-01-01\", \"2018-12-31\")\n",
    "df_test[\"date\"] = pd.date_range(\"2019-01-01\", \"2019-03-31\")\n",
    "\n",
    "# create key so we can cross join dates with pdv & artc datasets\n",
    "df_train[\"key\"] = 1\n",
    "df_test[\"key\"] = 1\n",
    "df_orders[\"key\"] = 1\n",
    "\n",
    "df_unique_pdv = df_orders[[\"key\", \"id_pdv\"]].drop_duplicates()\n",
    "df_unique_artc = df_orders[[\"key\", \"id_artc\"]].drop_duplicates()\n",
    "\n",
    "df_train = pd.merge(df_train, df_unique_pdv, on=\"key\")\n",
    "df_train = pd.merge(df_train, df_unique_artc, on=\"key\")\n",
    "\n",
    "df_test = pd.merge(df_test, df_unique_pdv, on=\"key\")\n",
    "df_test = pd.merge(df_test, df_unique_artc, on=\"key\")\n",
    "\n",
    "del df_train[\"key\"]\n",
    "del df_test[\"key\"]\n",
    "del df_orders[\"key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add orders to df_train\n",
    "df_orders[\"date\"] = pd.to_datetime(df_orders[\"date\"])\n",
    "\n",
    "df_train = pd.merge(df_train,\n",
    "                    df_orders,\n",
    "                    on=[\"date\", \"id_pdv\", \"id_artc\"],\n",
    "                    how=\"left\")\n",
    "\n",
    "df_train[\"qte\"] = df_train[\"qte\"].fillna(0).map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate last-7-days orders and remove empty last-7-days orders\n",
    "# note: this cell takes several minutes to execute\n",
    "df_train = df_train.sort_values(\"date\", ascending=True)\n",
    "df_train[\"qte_7_days\"] = df_train.groupby([\"id_pdv\", \"id_artc\"])[\"qte\"].transform(lambda x: x.rolling(7, min_periods=1).sum())\n",
    "df_train = df_train[df_train[\"qte_7_days\"] > 0].copy()\n",
    "del df_train[\"qte_7_days\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manomano/.pyenv/versions/3.7.0/envs/im_challenge/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/manomano/.pyenv/versions/3.7.0/envs/im_challenge/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "df_train[\"weekday\"] = df_train[\"date\"].dt.weekday\n",
    "df_test[\"weekday\"] = df_test[\"date\"].dt.weekday\n",
    "\n",
    "df_train[\"week\"] = df_train[\"date\"].dt.week\n",
    "df_test[\"week\"] = df_test[\"date\"].dt.week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefine week feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine week as the number of weeks since 01.01.2018\n",
    "df_test[\"week\"] += 52\n",
    "\n",
    "# with this definition, the last day of 2018 should be considered as week 53\n",
    "df_train.loc[df_train[\"date\"] == \"2018-12-31\", \"week\"] = 53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create log target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"y\"] = np.log1p(df_train[\"qte\"])\n",
    "df_test[\"y\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_train_dataset(df):\n",
    "    \"\"\"\n",
    "    After manual investigation, I noticed that:\n",
    "    - some stores were bugged on some days (missing or abnormal quantity sold)\n",
    "    - public holidays (1st of May, 8th of May, etc.) were disrupting the model\n",
    "    I decided to either remove some rows or \"fix\" somes rows using W+1 and W-1 data\n",
    "    \"\"\"\n",
    "\n",
    "    # id_pdv = 109 => remove bugged days from 06.05.2018 to 12.06.2018\n",
    "    df = df[(df[\"id_pdv\"] != 109) | ((df.date <= \"2018-05-06\") | (df.date >= \"2018-06-11\"))].copy()\n",
    "\n",
    "    # id_pdv = 115 => remove bugged days from 06.05.2018 to 12.06.2018\n",
    "    df = df[(df[\"id_pdv\"] != 115) | (df.date >= \"2018-01-29\")].copy()\n",
    "\n",
    "    # calculate W+1 and W-1 target to fix public holidays and bugged days\n",
    "    df[\"week_p1\"] = df[\"week\"] + 1\n",
    "    df[\"week_m1\"] = df[\"week\"] - 1\n",
    "    df = pd.merge(df,\n",
    "                  df[[\"week_p1\", \"weekday\", \"id_pdv\", \"id_artc\", \"y\"]].rename(columns={\"week_p1\":\"week\",\n",
    "                                                                                       \"y\": \"y_m1\"}),\n",
    "                  how=\"left\",\n",
    "                  on=[\"week\", \"weekday\", \"id_pdv\", \"id_artc\"])\n",
    "    df = pd.merge(df,\n",
    "                  df[[\"week_m1\", \"weekday\", \"id_pdv\", \"id_artc\", \"y\"]].rename(columns={\"week_m1\":\"week\",\n",
    "                                                                                       \"y\": \"y_p1\"}),\n",
    "                  how=\"left\",\n",
    "                  on=[\"week\", \"weekday\", \"id_pdv\", \"id_artc\"])\n",
    "\n",
    "    # fix bugged days\n",
    "    df = _fix_pdv_dates(df, list_id_pdv=[2, 5, 51, 52, 53, 55], list_dates=[\"2018-03-30\"])\n",
    "    df = _fix_pdv_dates(df, list_id_pdv=[25], list_dates=[\"2018-12-08\"])\n",
    "    df = _fix_pdv_dates(df, list_id_pdv=[50], list_dates=[\"2018-01-14\", \"2018-09-16\", \"2018-11-25\"])\n",
    "    df = _fix_pdv_dates(df, list_id_pdv=[59], list_dates=[\"2018-06-04\"])\n",
    "    df = _fix_pdv_dates(df, list_id_pdv=[69, 86], list_dates=[\"2018-11-17\", \"2018-11-18\"], wp1_only=True) # use W+1 only since W-1 is public holidays\n",
    "    df = _fix_pdv_dates(df, list_id_pdv=[100], list_dates=[\"2018-03-01\", \"2018-03-02\"])\n",
    "    df = _fix_pdv_dates(df, list_id_pdv=[109], list_dates=[\"2018-10-14\", \"2018-10-15\", \"2018-10-16\"])\n",
    "    df = _fix_pdv_dates(df, list_id_pdv=[135], list_dates=[\"2018-09-03\"])\n",
    "    df = _fix_pdv_dates(df, list_id_pdv=[140], list_dates=[\"2018-02-28\"])\n",
    "    \n",
    "\n",
    "    # fix public holidays\n",
    "    for d in [\"2018-04-02\", \"2018-05-10\", \"2018-05-21\", \"2018-07-14\", \"2018-08-15\", \"2018-11-01\", \"2018-11-11\"]:\n",
    "        df.loc[df[\"date\"] == d, \"y\"] = (0.5*df.loc[df[\"date\"] == d, \"y_m1\"] +\n",
    "                                        0.5*df.loc[df[\"date\"] == d, \"y_p1\"]) \n",
    "\n",
    "    for d in [\"2018-04-30\", \"2018-05-01\", \"2018-05-02\"]:\n",
    "        df.loc[df[\"date\"] == d, \"y\"] = df.loc[df[\"date\"] == d, \"y_m1\"]\n",
    "\n",
    "    for d in [\"2018-05-07\", \"2018-05-08\", \"2018-05-09\"]:\n",
    "        df.loc[df[\"date\"] == d, \"y\"] = df.loc[df[\"date\"] == d, \"y_p1\"]\n",
    "        \n",
    "    return df\n",
    "\n",
    "def _fix_pdv_dates(df, list_id_pdv, list_dates, wp1_only=False):\n",
    "    \"\"\"\n",
    "    For each pdv X date: fix the target using W-1 and W+1 data\n",
    "    If wp1_only is set to True, only use W+1 (ignore W-1)\n",
    "    \"\"\"\n",
    "    for id_pdv in list_id_pdv:\n",
    "        for date in list_dates:\n",
    "            filtre = (df[\"id_pdv\"] == id_pdv) & (df[\"date\"] == date)\n",
    "            if wp1_only:\n",
    "                df.loc[filtre , \"y\"] = df.loc[filtre, \"y_p1\"]\n",
    "            else:\n",
    "                df.loc[filtre , \"y\"] = (0.5*df.loc[filtre, \"y_m1\"] +\n",
    "                                        0.5*df.loc[filtre, \"y_p1\"]) \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = clean_train_dataset(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate df_train and df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_full = pd.concat((df_train, df_test), axis=0, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change granularity to week X id_pdv X id_artc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_week = pd.pivot_table(df_full,\n",
    "                              index=[\"id_pdv\", \"id_artc\", \"week\"],\n",
    "                              columns=\"weekday\",\n",
    "                              values=\"y\")\n",
    "\n",
    "df_full_week.columns = [f\"y{x}\" for x in df_full_week.columns]\n",
    "df_full_week = df_full_week.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save target columns\n",
    "columns_target = [col for col in df_full_week.columns if col.startswith(\"y\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NAs target\n",
    "df_full_week = df_full_week.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add pdv & artc metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_week = pd.merge(df_full_week, df_nom, on=\"id_artc\")\n",
    "df_full_week = pd.merge(df_full_week, df_pdv, on=\"id_pdv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add target at granularity week X weekday X id_pdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdv = df_full_week.groupby([\"week\", \"id_pdv\"])[columns_target].transform(np.mean)\n",
    "df_pdv.columns = [x+\"_pdv\" for x in df_pdv.columns]\n",
    "df_full_week = pd.concat((df_full_week, df_pdv), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add target at granularity week X weekday X id_artc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artc = df_full_week.groupby([\"week\", \"id_artc\"])[columns_target].transform(np.mean)\n",
    "df_artc.columns = [x+\"_artc\" for x in df_artc.columns]\n",
    "df_full_week = pd.concat((df_full_week, df_artc), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create quarter feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: since the dataset is at week granularity, we can't use the dt.quarter function\n",
    "df_full_week[\"real_week\"] = df_full_week[\"week\"]\n",
    "df_full_week.loc[df_full_week[\"real_week\"] >= 53, \"real_week\"] -= 52\n",
    "df_full_week[\"quarter\"] = 4\n",
    "df_full_week.loc[df_full_week.real_week <= 39, 'quarter'] = 3\n",
    "df_full_week.loc[df_full_week.real_week <= 26, 'quarter'] = 2\n",
    "df_full_week.loc[df_full_week.real_week <= 13, 'quarter'] = 1\n",
    "del df_full_week[\"real_week\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Modelling <a name=3></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "import keras\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "def _get_te_feature_mapping(df_week, group):\n",
    "    \"\"\"\n",
    "    Get the mapping dataframe between the group columns and the target columns\n",
    "    (that can be joined to the main dataframe)\n",
    "    \"\"\"\n",
    "    if \"weekday\" in group:\n",
    "        group_wout_weekday = [c for c in group if c != \"weekday\"]\n",
    "        df_mapping = df_week.groupby(group_wout_weekday)[columns_target].mean()\n",
    "        df_mapping.columns = [\"x_te_\"+\"_\".join(group_wout_weekday)+\"_\"+x for x in df_mapping.columns]\n",
    "        df_mapping = df_mapping.reset_index()\n",
    "    else:\n",
    "        df_mapping = df_week.groupby(group)[columns_target].mean().mean(axis=1)\n",
    "        df_mapping = df_mapping.reset_index()\n",
    "        df_mapping.columns = group + [\"x_te_\"+'_'.join(group)]\n",
    "        \n",
    "    return df_mapping\n",
    "\n",
    "def add_te_features(X, df, group, min_week, max_week, weeks_to_ignore):\n",
    "    \"\"\"\n",
    "    Target encode within group columns. Features are:\n",
    "    - calculated between min_week and max_week\n",
    "    - calculated ignoring weeks_to_ignore weeks\n",
    "    - concatenated to X dataframe\n",
    "    \"\"\"\n",
    "    df_mapping = _get_te_feature_mapping(df[(df[\"week\"] <= max_week) &\n",
    "                                           (df[\"week\"] >= min_week) & \n",
    "                                           (~df[\"week\"].isin(weeks_to_ignore))], group)\n",
    "    \n",
    "    list_col_merge = [x for x in group if x != \"weekday\"]\n",
    "    \n",
    "    X = pd.merge(X,\n",
    "                 df_mapping,\n",
    "                 on=list_col_merge,\n",
    "                 how=\"left\")\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL:nn - WEEK_PREDICT:1\n",
      "(52713, 241) (1831, 241) 209\n",
      "MODEL:nn - WEEK_PREDICT:4\n",
      "(49202, 241) (1831, 241) 209\n",
      "MODEL:nn - WEEK_PREDICT:7\n",
      "(45645, 241) (1831, 241) 209\n",
      "MODEL:nn - WEEK_PREDICT:10\n",
      "(42024, 241) (1831, 241) 209\n",
      "MODEL:nn - WEEK_PREDICT:13\n",
      "(38418, 241) (1831, 241) 209\n",
      "MODEL:gbm - WEEK_PREDICT:1\n",
      "(52713, 241) (1831, 241) 209\n",
      "MODEL:gbm - WEEK_PREDICT:4\n",
      "(49202, 241) (1831, 241) 209\n",
      "MODEL:gbm - WEEK_PREDICT:7\n",
      "(45645, 241) (1831, 241) 209\n",
      "MODEL:gbm - WEEK_PREDICT:10\n",
      "(42024, 241) (1831, 241) 209\n",
      "MODEL:gbm - WEEK_PREDICT:13\n",
      "(38418, 241) (1831, 241) 209\n"
     ]
    }
   ],
   "source": [
    "# number of weeks used to calculate historical target values (at different granularities)\n",
    "NB_WEEKS_HISTORIC = 6\n",
    "\n",
    "# number of weeks we want to ignore in december\n",
    "LAG = 1 \n",
    "\n",
    "# lists that will contain prediction dataframes\n",
    "list_df_preds_nn = []\n",
    "list_df_preds_gbm = []\n",
    "\n",
    "def nn_model():\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add(keras.layers.Dense(256, activation='relu', input_dim=input_dims))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(output_dims, activation='linear'))\n",
    "    \n",
    "    model.compile(loss=\"mse\",\n",
    "                  optimizer=keras.optimizers.Adam(lr=0.00005),\n",
    "                  metrics=[[keras.metrics.RootMeanSquaredError(name='rmse')]])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# iterate over models (nn and gbm) and weeks\n",
    "# note: since the predictions are blended at the end and are the same for every week, \n",
    "# we only iterate over 5 weeks to make the program run faster\n",
    "for MODEL in [\"nn\", \"gbm\"]:\n",
    "    for WEEK_PREDICT in [1, 4, 7, 10, 13]:\n",
    "        \n",
    "        print(f\"MODEL:{MODEL} - WEEK_PREDICT:{WEEK_PREDICT}\")\n",
    "\n",
    "        # remove rows on which we can't have full historic\n",
    "        X = df_full_week[df_full_week[\"week\"] >= LAG + WEEK_PREDICT + NB_WEEKS_HISTORIC].copy()\n",
    "\n",
    "        # historical target values at different granularities\n",
    "        X[\"week_saved\"] = X[\"week\"]\n",
    "        for nb_weeks_lag in range(LAG+WEEK_PREDICT, LAG+WEEK_PREDICT+NB_WEEKS_HISTORIC):\n",
    "            X[\"week\"] = X[\"week_saved\"] - nb_weeks_lag\n",
    "\n",
    "            X = pd.merge(X,\n",
    "                         df_full_week[[\"id_pdv\", \"id_artc\", \"week\"]+[c for c in df_full_week.columns if c.startswith(\"y\")]],\n",
    "                         on=[\"id_pdv\", \"id_artc\", \"week\"],\n",
    "                         how=\"left\",\n",
    "                         suffixes=(\"\", f\"_{nb_weeks_lag}_x\"))\n",
    "\n",
    "            # don't use the last 3 days of W51 (this was tuned locally)\n",
    "            if nb_weeks_lag == (LAG+WEEK_PREDICT):\n",
    "                to_keep = [c for c in X.columns if c not in [f'y4_{nb_weeks_lag}_x',\n",
    "                                                             f'y5_{nb_weeks_lag}_x',\n",
    "                                                             f'y6_{nb_weeks_lag}_x',\n",
    "                                                             f'y4_pdv_{nb_weeks_lag}_x',\n",
    "                                                             f'y5_pdv_{nb_weeks_lag}_x',\n",
    "                                                             f'y6_pdv_{nb_weeks_lag}_x',\n",
    "                                                             f'y4_artc_{nb_weeks_lag}_x',\n",
    "                                                             f'y5_artc_{nb_weeks_lag}_x',\n",
    "                                                             f'y6_artc_{nb_weeks_lag}_x']]\n",
    "                X = X[to_keep]\n",
    "\n",
    "        X[\"week\"] = X[\"week_saved\"]\n",
    "        del X[\"week_saved\"]\n",
    "\n",
    "        # add target encoding features   \n",
    "        for group in [[\"weekday\", \"lb_vent_faml\"],\n",
    "                      [\"weekday\", \"id_artc\"],\n",
    "                      [\"weekday\", \"id_pdv\"],\n",
    "                      [\"weekday\", \"id_voct\"],\n",
    "                      [\"weekday\", \"id_artc\", \"id_pdv\"],\n",
    "                      [\"weekday\", \"id_voct\", \"nb_cais_grp\"],\n",
    "                      ['weekday', 'id_pdv', 'lb_vent_faml'],\n",
    "                      ['weekday', 'lb_vent_faml', 'nb_cais_grp'],\n",
    "                      [\"lb_vent_faml\", \"id_regn\", \"nb_cais_grp\"],\n",
    "                      [\"weekday\", \"id_voct\", \"id_artc\"],\n",
    "                      [\"weekday\", \"id_regn\", \"id_artc\"],\n",
    "                      ['weekday', 'lb_vent_sous_faml'],\n",
    "                      ['weekday', 'quarter', \"id_pdv\"],\n",
    "                      ['weekday', 'quarter', \"lb_vent_faml\"]]:\n",
    "            \n",
    "            # ignore some weeks to calculate target encoding features\n",
    "            if MODEL == \"nn\":\n",
    "                weeks_to_ignore = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n",
    "                                   16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,\n",
    "                                   29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 51, 52]\n",
    "            elif MODEL == \"gbm\":\n",
    "                weeks_to_ignore = [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
    "                                   25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
    "                                   37, 38, 51, 52]\n",
    "\n",
    "            X = add_te_features(X,\n",
    "                                df_full_week,\n",
    "                                group,\n",
    "                                min_week=0,\n",
    "                                max_week=50,\n",
    "                                weeks_to_ignore=weeks_to_ignore)\n",
    "\n",
    "        # fill NAs added by target encoding features\n",
    "        X = X.fillna(0)\n",
    "\n",
    "        # train/test split\n",
    "        X_train = X[X[\"week\"] <= 50]\n",
    "        X_test = X[X[\"week\"] == (52 + WEEK_PREDICT)].copy()  \n",
    "        y_train = X_train[columns_target]\n",
    "\n",
    "        # create list of features\n",
    "        features = [c for c in X_train.columns if c.endswith(\"_x\") or c.startswith(\"x_te_\")]\n",
    "        \n",
    "        print(X_train.shape, X_test.shape, len(features))\n",
    "\n",
    "        if MODEL == \"nn\":\n",
    "            df_preds = X_test[[\"id_pdv\", \"id_artc\", \"week\"]].copy()\n",
    "            \n",
    "            input_dims = len(features)\n",
    "            output_dims = 7\n",
    "\n",
    "            # initiate nn\n",
    "            nn = KerasRegressor(build_fn=nn_model)\n",
    "            \n",
    "            # fit\n",
    "            history = nn.fit(\n",
    "                X_train[features].values, y_train.values, \n",
    "                epochs=10,\n",
    "                batch_size=1000,\n",
    "                verbose=0,\n",
    "                shuffle=True  \n",
    "            )\n",
    "\n",
    "            # predict\n",
    "            preds = nn.predict(X_test[features])\n",
    "            \n",
    "            for i in range(7):\n",
    "                df_preds[i] = preds[:, i]\n",
    "\n",
    "            list_df_preds_nn.append(df_preds)\n",
    "\n",
    "        elif MODEL == \"gbm\":\n",
    "            df_preds = X_test[[\"id_pdv\", \"id_artc\", \"week\"]].copy()\n",
    "\n",
    "            # 1 model by weekday\n",
    "            for i in range(7):\n",
    "                target = \"y\"+str(i)\n",
    "                \n",
    "                # initiate lgb\n",
    "                lgb = LGBMRegressor(nthread=16,\n",
    "                                    objective='rmse',\n",
    "                                    n_estimators=100,\n",
    "                                    num_leaves=21,\n",
    "                                    learning_rate=0.05,\n",
    "                                    subsample=0.8,\n",
    "                                    subsample_freq=1,\n",
    "                                    colsample_bytree=0.3,\n",
    "                                    min_child_samples=30,\n",
    "                                    random_state=SEED)\n",
    "                \n",
    "                # fit\n",
    "                lgb.fit(X_train[features],\n",
    "                        y_train[target])\n",
    "                \n",
    "                # predict\n",
    "                df_preds[i] = lgb.predict(X_test[features])\n",
    "\n",
    "            list_df_preds_gbm.append(df_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Post-processing <a name=4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate average predictions by weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_gbm = pd.concat(list_df_preds_gbm, axis=0)\n",
    "df_preds_nn = pd.concat(list_df_preds_nn, axis=0)\n",
    "\n",
    "df_preds_gbm = df_preds_gbm.melt(id_vars=[\"id_pdv\", \"id_artc\", \"week\"], var_name=[\"weekday\"], value_name=\"pred\")\n",
    "df_preds_nn = df_preds_nn.melt(id_vars=[\"id_pdv\", \"id_artc\", \"week\"], var_name=[\"weekday\"], value_name=\"pred\")\n",
    "\n",
    "df_preds_gbm = df_preds_gbm.groupby([\"id_pdv\", \"id_artc\", \"weekday\"])[\"pred\"].mean().reset_index()\n",
    "df_preds_nn = df_preds_nn.groupby([\"id_pdv\", \"id_artc\", \"weekday\"])[\"pred\"].mean().reset_index()\n",
    "\n",
    "df_preds_gbm = pd.merge(df_test, df_preds_gbm, on=[\"id_pdv\", \"id_artc\", \"weekday\"], how=\"left\")\n",
    "df_preds_nn = pd.merge(df_test, df_preds_nn, on=[\"id_pdv\", \"id_artc\", \"weekday\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging NN and GBM predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.merge(df_preds_gbm, df_preds_nn, on=[\"date\", \"id_pdv\", \"id_artc\"])\n",
    "\n",
    "df_preds[\"pred\"] = 0.5*df_preds[\"pred_x\"] + 0.5*df_preds[\"pred_y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds[\"qte\"] = np.exp(df_preds[\"pred\"]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.loc[df_preds[\"qte\"] < 0, \"qte\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast to int (choose between floor and ceil by minimizing MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate squared error for floor & ceil casts\n",
    "df_preds[\"mse_floor\"] = df_preds[\"qte\"].map(lambda x: (np.log1p(x) - np.log1p(np.floor(x)))**2)\n",
    "df_preds[\"mse_ceil\"] = df_preds[\"qte\"].map(lambda x: (np.log1p(x) - np.log1p(np.ceil(x)))**2)\n",
    "\n",
    "# choose between the cast with the lowest squared error\n",
    "filtre = df_preds[\"mse_floor\"] < df_preds[\"mse_ceil\"]\n",
    "df_preds.loc[filtre, \"qte\"] = np.floor(df_preds.loc[filtre, \"qte\"])\n",
    "df_preds.loc[~filtre, \"qte\"] = np.ceil(df_preds.loc[~filtre, \"qte\"])\n",
    "\n",
    "# cast dtype to int\n",
    "df_preds[\"qte\"] = df_preds[\"qte\"].map(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post processing prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this list was created manually\n",
    "# I suppose these stores are usually closed on sunday by looking at their historical data\n",
    "# this could be industrialized knowing the stores opening schedule\n",
    "id_pdv_not_opened_on_sunday = [12, 128, 149, 110, 102, 14, 75,\n",
    "                               139, 38, 127, 4, 33, 73, 130, 52,\n",
    "                               98, 129, 140, 96, 87, 5, 51,\n",
    "                               55, 53, 146, 2, 65, 89]\n",
    "\n",
    "df_preds[\"date\"] = pd.to_datetime(df_preds[\"date\"])\n",
    "df_preds[\"weekday\"] = df_preds[\"date\"].dt.weekday\n",
    "\n",
    "# force \"supposed closed pdv on sunday\" predictions to 0\n",
    "df_preds.loc[(df_preds[\"id_pdv\"].isin(id_pdv_not_opened_on_sunday)) & (df_preds[\"weekday\"] == 6), \"qte\"] = 0\n",
    "\n",
    "# force 1st of January predictions to 0\n",
    "df_preds.loc[df_preds[\"date\"] == \"2019-01-01\", \"qte\"] = 0\n",
    "\n",
    "# keep only rows where qte > 0\n",
    "df_preds = df_preds[df_preds[\"qte\"] > 0].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Create & dump submission file <a name=5></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_SUB = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: submits/1.csv (deflated 85%)\n"
     ]
    }
   ],
   "source": [
    "df_preds[\"id\"] = (df_preds[\"id_pdv\"].map(str) + \"_\"\n",
    "                  + df_preds[\"id_artc\"].map(str) + \"_\"\n",
    "                  + df_preds[\"date\"].dt.strftime('%Y%m%d'))\n",
    "\n",
    "df_preds[[\"id\", \"qte\"]].to_csv(f\"submits/{ID_SUB}.csv\", index=False)\n",
    "\n",
    "!zip submits/{ID_SUB}.zip submits/{ID_SUB}.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "im_challenge",
   "language": "python",
   "name": "im_challenge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
